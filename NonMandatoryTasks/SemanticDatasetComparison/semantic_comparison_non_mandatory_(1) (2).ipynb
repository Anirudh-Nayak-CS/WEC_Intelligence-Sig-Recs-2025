{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 13489108,
          "sourceType": "datasetVersion",
          "datasetId": 8564304
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## research paper used  -> https://arxiv.org/pdf/2103.00020 ,https://arxiv.org/html/2406.03865v1 (used this to get an idea of semantic similarity)\n"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "ForV_NP5_2Yc"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLIP"
      ],
      "metadata": {
        "id": "wbA30ecX_2Ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision  import datasets,transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", dtype=torch.float16, attn_implementation=\"sdpa\")\n",
        "processor =CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\",use_fast=True)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZPf9D-E_2Yf",
        "outputId": "7dada8b1-2d5a-44bb-948b-199e1b30ba9b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T15:11:04.259531Z",
          "iopub.execute_input": "2025-10-24T15:11:04.259851Z",
          "iopub.status.idle": "2025-10-24T15:11:08.403159Z",
          "shell.execute_reply.started": "2025-10-24T15:11:04.259828Z",
          "shell.execute_reply": "2025-10-24T15:11:08.401801Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIPModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 512)\n",
              "      (position_embedding): Embedding(77, 512)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (vision_model): CLIPVisionTransformer(\n",
              "    (embeddings): CLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (position_embedding): Embedding(50, 768)\n",
              "    )\n",
              "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "13wpaQ4eBYOO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T14:52:56.735455Z",
          "iopub.status.idle": "2025-10-24T14:52:56.735765Z",
          "shell.execute_reply.started": "2025-10-24T14:52:56.735627Z",
          "shell.execute_reply": "2025-10-24T14:52:56.735641Z"
        }
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp 1 (CIFAR SET)"
      ],
      "metadata": {
        "id": "dIVsFyTuHUX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dataset=torchvision.datasets.CIFAR10('data',train=True,download=True,target_transform=None,transform=None)\n"
      ],
      "metadata": {
        "id": "QE8o2kpW_2Yf"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxbX-LIe_7Y9",
        "outputId": "50e6c12c-67d2-4fbb-bd92-d359dec11fac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "classes=train_dataset.classes\n",
        "classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QxBYMm5AyRb",
        "outputId": "4bbe6645-805f-46c7-9405-a019f1a7352c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['airplane',\n",
              " 'automobile',\n",
              " 'bird',\n",
              " 'cat',\n",
              " 'deer',\n",
              " 'dog',\n",
              " 'frog',\n",
              " 'horse',\n",
              " 'ship',\n",
              " 'truck']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.class_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbCQfeWEBHFW",
        "outputId": "b90e0dd2-3726-44fe-ec87-69af32d1c750"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'airplane': 0,\n",
              " 'automobile': 1,\n",
              " 'bird': 2,\n",
              " 'cat': 3,\n",
              " 'deer': 4,\n",
              " 'dog': 5,\n",
              " 'frog': 6,\n",
              " 'horse': 7,\n",
              " 'ship': 8,\n",
              " 'truck': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "img,label_idx=train_dataset[2]\n",
        "label=classes[label_idx]\n"
      ],
      "metadata": {
        "id": "vvafUDjQAz0c"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(label)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "5nWh5AvYBMMa",
        "outputId": "59377621-85a3-4681-c1e3-83f54bf14c01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7e8a4d01d370>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "text_prompt = [f\"a photo of a {label}\" for label in classes]\n",
        "inputs = processor(\n",
        "    text=text_prompt,\n",
        "    images=[img],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "1S7Nw9atBcRv"
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity score between cat and dog"
      ],
      "metadata": {
        "id": "xkBQjIR6KHpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_a_images=[] # dog\n",
        "dataset_b_images=[] # cat\n",
        "dataset_c_images=[] #truck\n",
        "dataset_d_images=[] #ship\n",
        "\n",
        "for (img,label) in train_dataset:\n",
        "  if label==5:\n",
        "    dataset_a_images.append(img)\n",
        "  elif label==3:\n",
        "    dataset_b_images.append(img)\n",
        "  elif label==9:\n",
        "    dataset_c_images.append(img)\n",
        "  elif label==8:\n",
        "    dataset_d_images.append(img)\n"
      ],
      "metadata": {
        "id": "0mEEATP3CURm"
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "JPwe_0fL1Dfh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_pil(batch):\n",
        "  images = []\n",
        "  for item in batch:\n",
        "     img=item\n",
        "     images.append(img)\n",
        "\n",
        "\n",
        "  return images"
      ],
      "metadata": {
        "id": "Vj1p8P5E8zz9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self,images):\n",
        "    self.images=images\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.images[idx]\n",
        "\n",
        "dog_dataset=ImageDataset(dataset_a_images)\n",
        "cat_dataset=ImageDataset(dataset_b_images)\n",
        "truck_dataset=ImageDataset(dataset_c_images)\n",
        "ship_dataset=ImageDataset(dataset_d_images)\n",
        "\n",
        "dog_loader = DataLoader(dog_dataset, batch_size=32, shuffle=False,collate_fn=custom_collate_pil)\n",
        "cat_loader = DataLoader(cat_dataset, batch_size=32, shuffle=False,collate_fn=custom_collate_pil)\n",
        "truck_loader = DataLoader(truck_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_pil)\n",
        "ship_loader = DataLoader(ship_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_pil)"
      ],
      "metadata": {
        "id": "enXs-g6mLh2S",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T14:52:56.737043Z",
          "iopub.status.idle": "2025-10-24T14:52:56.737467Z",
          "shell.execute_reply.started": "2025-10-24T14:52:56.737249Z",
          "shell.execute_reply": "2025-10-24T14:52:56.737266Z"
        }
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_embeddings(dataloader,model,processor,device):\n",
        "  all_embeddings=[]\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images_batch in dataloader:\n",
        "\n",
        "            inputs = processor(\n",
        "                images=images_batch,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "\n",
        "\n",
        "            image_features = model.get_image_features(**inputs)\n",
        "\n",
        "\n",
        "            all_embeddings.append(image_features.cpu())\n",
        "\n",
        "    return torch.cat(all_embeddings, dim=0)\n"
      ],
      "metadata": {
        "id": "BxFhKlrYKsux"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "dog_embeddings=extract_embeddings(dog_loader,model,processor,device)\n",
        "cat_embeddings=extract_embeddings(cat_loader,model,processor,device)\n",
        "truck_embeddings=extract_embeddings(truck_loader,model,processor,device)\n",
        "ship_embeddings=extract_embeddings(ship_loader,model,processor,device)"
      ],
      "metadata": {
        "id": "GCufswTrUrO5"
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity scores  \n"
      ],
      "metadata": {
        "id": "W35PfTHQU8N8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CAT VS DOG"
      ],
      "metadata": {
        "id": "cQnF-3no7glP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Centroid Cosine similarity"
      ],
      "metadata": {
        "id": "BtOqha5mVMZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "dog_embeddings_cpy=dog_embeddings\n",
        "cat_embeddings_cpy=cat_embeddings\n",
        "dog_mean=torch.mean(dog_embeddings,dim=0)  # shape [512]\n",
        "cat_mean=torch.mean(cat_embeddings,dim=0)  # shape [512]\n",
        "\n",
        "cosine_similarity_score=F.cosine_similarity(    # we unsqueze because 2d vector is exprected so we get  -> [1,512]\n",
        "    dog_mean.unsqueeze(0),\n",
        "    cat_mean.unsqueeze(0),\n",
        ")\n",
        "print(f\"\\nCentroid Cosine Similarity (Dog vs. Cat): {cosine_similarity_score.item():.4f}\")"
      ],
      "metadata": {
        "id": "1ebxrQt4U6nM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21362e8f-c0a0-44d3-b7db-4c19e4acc4af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Centroid Cosine Similarity (Dog vs. Cat): 0.9785\n"
          ]
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fréchet Embedding Distance"
      ],
      "metadata": {
        "id": "_VjRAhU4a-S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import sqrtm"
      ],
      "metadata": {
        "id": "tCaGBQpGbCPr",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T15:13:56.191940Z",
          "iopub.execute_input": "2025-10-24T15:13:56.192357Z",
          "iopub.status.idle": "2025-10-24T15:13:56.201720Z",
          "shell.execute_reply.started": "2025-10-24T15:13:56.192324Z",
          "shell.execute_reply": "2025-10-24T15:13:56.197115Z"
        }
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fed(embedding1,embedding2):\n",
        "  embedding1 = embedding1.cpu().numpy()\n",
        "  embedding2 = embedding2.cpu().numpy()\n",
        "  mean_1,cov_1=np.mean(embedding1,axis=1),np.cov(embedding1)\n",
        "  mean_2,cov_2=np.mean(embedding2,axis=1),np.cov(embedding2)\n",
        "\n",
        "  squared_diff_mean=np.sum((mean_1-mean_2)**2)\n",
        "  eps = 1e-6\n",
        "  covmean, _ = sqrtm(cov_1.dot(cov_2) + eps * np.eye(cov_1.shape[0]), disp=False)\n",
        "\n",
        "    # Matrix square root might return complex numbers, take the real part\n",
        "  if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "  fed=squared_diff_mean+np.trace(cov_1+cov_2-2.0*covmean)\n",
        "  fed=max(0,fed)\n",
        "  return fed"
      ],
      "metadata": {
        "id": "Hyf-lluy5f97",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T15:14:00.277995Z",
          "iopub.execute_input": "2025-10-24T15:14:00.278394Z",
          "iopub.status.idle": "2025-10-24T15:14:00.285039Z",
          "shell.execute_reply.started": "2025-10-24T15:14:00.278360Z",
          "shell.execute_reply": "2025-10-24T15:14:00.283783Z"
        }
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "fed_score = calculate_fed(cat_embeddings_cpy, dog_embeddings_cpy)\n",
        "print(f\"Fréchet Embedding Distance: {fed_score:.4f}\")"
      ],
      "metadata": {
        "id": "zVpvFu9_6ld-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96a8f5f-8229-4441-9e0b-8520f2a0108d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1256775283.py:9: DeprecationWarning: The `disp` argument is deprecated and will be removed in SciPy 1.18.0.\n",
            "  covmean, _ = sqrtm(cov_1.dot(cov_2) + eps * np.eye(cov_1.shape[0]), disp=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fréchet Embedding Distance: 347.6880\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DOG VS TRUCK"
      ],
      "metadata": {
        "id": "1lNv6MSs7jMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Centroid Cosine similarity"
      ],
      "metadata": {
        "id": "ZWVKdSqu7mi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "dog_mean=torch.mean(dog_embeddings,dim=0)  # shape [512]\n",
        "truck_mean=torch.mean(truck_embeddings,dim=0)  # shape [512]\n",
        "\n",
        "cosine_similarity_score=F.cosine_similarity(    # we unsqueze because 2d vector is exprected so we get  -> [1,512]\n",
        "    dog_mean.unsqueeze(0),\n",
        "    truck_mean.unsqueeze(0),\n",
        ")\n",
        "print(f\"\\nCentroid Cosine Similarity (Dog vs. Truck): {cosine_similarity_score.item():.4f}\")"
      ],
      "metadata": {
        "id": "jb6F0FR27kkH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd1b91b7-8567-4c05-b4fe-f55a7a522962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Centroid Cosine Similarity (Dog vs. Truck): 0.8545\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Frechet embedding distance"
      ],
      "metadata": {
        "id": "ewjB1lb77o74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fed_score = calculate_fed(truck_embeddings, dog_embeddings)\n",
        "print(f\"Fréchet Embedding Distance: {fed_score:.4f}\")"
      ],
      "metadata": {
        "id": "zCicSTyg7rMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276b49a4-a205-45c2-9910-ac61bb934633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1256775283.py:9: DeprecationWarning: The `disp` argument is deprecated and will be removed in SciPy 1.18.0.\n",
            "  covmean, _ = sqrtm(cov_1.dot(cov_2) + eps * np.eye(cov_1.shape[0]), disp=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fréchet Embedding Distance: 370.8711\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TRUCK VS SHIP"
      ],
      "metadata": {
        "id": "Y773BpjG8HOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Centroid cosine similarity"
      ],
      "metadata": {
        "id": "tng8zHo58Jkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "ship_mean=torch.mean(ship_embeddings,dim=0)  # shape [512]\n",
        "truck_mean=torch.mean(truck_embeddings,dim=0)  # shape [512]\n",
        "\n",
        "cosine_similarity_score=F.cosine_similarity(    # we unsqueze because 2d vector is exprected so we get  -> [1,512]\n",
        "    ship_mean.unsqueeze(0),\n",
        "    truck_mean.unsqueeze(0),\n",
        ")\n",
        "print(f\"\\nCentroid Cosine Similarity (Ship vs. Truck): {cosine_similarity_score.item():.4f}\")"
      ],
      "metadata": {
        "id": "VaRrmVsR8Ipc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d229fd-136a-4eef-d485-cdea0b9dff89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Centroid Cosine Similarity (Ship vs. Truck): 0.9106\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Frechet embedding distance"
      ],
      "metadata": {
        "id": "H5ER1N2N8L5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fed_score = calculate_fed(truck_embeddings, ship_embeddings)\n",
        "print(f\"Fréchet Embedding Distance: {fed_score:.4f}\")"
      ],
      "metadata": {
        "id": "q874ENXI8N4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a39aab-66f5-490a-de81-914a12538941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1256775283.py:9: DeprecationWarning: The `disp` argument is deprecated and will be removed in SciPy 1.18.0.\n",
            "  covmean, _ = sqrtm(cov_1.dot(cov_2) + eps * np.eye(cov_1.shape[0]), disp=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fréchet Embedding Distance: 379.1478\n"
          ]
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXP_2 (CIFAKE)"
      ],
      "metadata": {
        "id": "0HQ9z5u098BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "zip_file_path = '/content/archive (2).zip'\n",
        "\n",
        "\n",
        "extract_directory = '/content/extracted_data'\n",
        "\n",
        "os.makedirs(extract_directory, exist_ok=True)\n",
        "\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_directory)\n",
        "    print(f\"Successfully extracted '{os.path.basename(zip_file_path)}' to '{extract_directory}'\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Zip file not found at '{zip_file_path}'. Please check the path.\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: The file at '{zip_file_path}' is not a valid zip file or is corrupted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCD3P81FLQT4",
        "outputId": "445518c7-9ab9-47f7-8347-a054772cd064"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully extracted 'archive (2).zip' to '/content/extracted_data'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "test_fake_path=\"/content/extracted_data/test/FAKE\"\n",
        "test_real_path=\"/content/extracted_data/test/REAL\"\n",
        "train_fake_path=\"/content/extracted_data/train/FAKE\"\n",
        "train_real_path=\"/content/extracted_data/train/REAL\"\n",
        "test_fake_imgs=[f\"{test_fake_path}/{filename}\" for filename in os.listdir(test_fake_path)]\n",
        "test_real_imgs=[f\"{test_real_path}/{filename}\"for filename in os.listdir(test_real_path)]\n",
        "train_fake_imgs=[f\"{train_fake_path}/{filename}\" for filename in os.listdir(train_fake_path)]\n",
        "train_real_imgs=[f\"{train_real_path}/{filename}\"for filename in os.listdir(train_real_path)]\n"
      ],
      "metadata": {
        "id": "U0KN20a59-RJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T14:52:18.368243Z",
          "iopub.execute_input": "2025-10-24T14:52:18.368516Z",
          "iopub.status.idle": "2025-10-24T14:52:18.596927Z",
          "shell.execute_reply.started": "2025-10-24T14:52:18.368495Z",
          "shell.execute_reply": "2025-10-24T14:52:18.595829Z"
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "import torchvision\n",
        "from torchvision  import datasets,transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from PIL import Image\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self,images):\n",
        "    self.images=images\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.images[idx]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T14:54:05.364145Z",
          "iopub.execute_input": "2025-10-24T14:54:05.365162Z",
          "iopub.status.idle": "2025-10-24T14:54:05.392367Z",
          "shell.execute_reply.started": "2025-10-24T14:54:05.365070Z",
          "shell.execute_reply": "2025-10-24T14:54:05.391089Z"
        },
        "id": "rhL4FNW4zBt-"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "train_fake_dataset=ImageDataset(train_fake_imgs)\n",
        "train_real_dataset=ImageDataset(train_real_imgs)\n",
        "test_fake_dataset=ImageDataset(test_fake_imgs)\n",
        "test_real_dataset=ImageDataset(test_real_imgs)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pBP8UlrCzBuE"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_fake_loader = DataLoader(train_fake_dataset, batch_size=32, shuffle=False,collate_fn=custom_collate_pil)\n",
        "train_real_loader = DataLoader(train_real_dataset, batch_size=32, shuffle=False,collate_fn=custom_collate_pil)\n",
        "test_fake_loader = DataLoader(test_fake_dataset, batch_size=32, shuffle=False,collate_fn=custom_collate_pil)\n",
        "test_real_loader = DataLoader(test_real_dataset, batch_size=32, shuffle=False,collate_fn=custom_collate_pil)"
      ],
      "metadata": {
        "trusted": true,
        "id": "FUpEzhb-zBuF"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_embeddings(dataloader,model,processor,device):\n",
        "  all_embeddings=[]\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images_batch in dataloader:\n",
        "\n",
        "            inputs = processor(\n",
        "                images=images_batch,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "\n",
        "\n",
        "            image_features = model.get_image_features(**inputs)\n",
        "\n",
        "\n",
        "            all_embeddings.append(image_features.cpu())\n",
        "\n",
        "    # Concatenate all batches into one big tensor\n",
        "    # The shape will be [num_images, embedding_dim] (e.g., [5000, 512])\n",
        "    return torch.cat(all_embeddings, dim=0)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T15:10:57.701710Z",
          "iopub.execute_input": "2025-10-24T15:10:57.702710Z",
          "iopub.status.idle": "2025-10-24T15:10:57.708103Z",
          "shell.execute_reply.started": "2025-10-24T15:10:57.702670Z",
          "shell.execute_reply": "2025-10-24T15:10:57.707102Z"
        },
        "id": "B_xjn9BUzBuF"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "train_fake_embeddings=extract_embeddings(train_fake_loader,model,processor,device)\n",
        "test_fake_embeddings=extract_embeddings(test_fake_loader,model,processor,device)\n",
        "train_real_embeddings=extract_embeddings(train_real_loader,model,processor,device)\n",
        "test_real_embeddings=extract_embeddings(test_real_loader,model,processor,device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bk-_FWEKzBuF"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity score"
      ],
      "metadata": {
        "id": "pOjDGVKpzBuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TRAIN ->  REAL VS FAKE"
      ],
      "metadata": {
        "id": "3G2ySeAyzBuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Centroid Cosine similarity"
      ],
      "metadata": {
        "id": "kpL1LL2xzBuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "train_real_mean=torch.mean(train_real_embeddings,dim=0)\n",
        "train_fake_mean=torch.mean(train_fake_embeddings,dim=0)\n",
        "\n",
        "cosine_similarity_score=F.cosine_similarity(\n",
        "    train_real_mean.unsqueeze(0),\n",
        "    train_fake_mean.unsqueeze(0),\n",
        ")\n",
        "print(f\"\\nCentroid Cosine Similarity: {cosine_similarity_score.item():.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEz6VtCYzBuG",
        "outputId": "3616c820-b1ee-4c71-8028-c9f5c6446c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Centroid Cosine Similarity: 0.9912\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fréchet Embedding Distance"
      ],
      "metadata": {
        "id": "ciO90R1ZzBuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "pDOiVhxYQgDI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_real_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7OmNMx-RVrP",
        "outputId": "e9af5623-53c9-45f4-a4bc-834a5a43ad13"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fed_score = calculate_fed(train_real_embeddings[:2000], train_fake_embeddings[:2000])\n",
        "print(f\"Fréchet Embedding Distance: {fed_score:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0gH_bdVzBuG",
        "outputId": "271b0373-b3da-4288-ea42-2139c820bdb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1256775283.py:9: DeprecationWarning: The `disp` argument is deprecated and will be removed in SciPy 1.18.0.\n",
            "  covmean, _ = sqrtm(cov_1.dot(cov_2) + eps * np.eye(cov_1.shape[0]), disp=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fréchet Embedding Distance: 170.8972\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST -> REAL VS FAKE"
      ],
      "metadata": {
        "id": "3_ZiFzTMzBuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Centroid Cosine similarity"
      ],
      "metadata": {
        "id": "HxNbHcqizBuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "test_fake_mean=torch.mean(test_fake_embeddings,dim=0)\n",
        "test_real_mean=torch.mean(test_real_embeddings,dim=0)\n",
        "\n",
        "cosine_similarity_score=F.cosine_similarity(\n",
        "    test_real_mean.unsqueeze(0),\n",
        "    test_fake_mean.unsqueeze(0),\n",
        ")\n",
        "print(f\"\\nCentroid Cosine Similarity : {cosine_similarity_score.item():.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ap4niqDzBuH",
        "outputId": "10b53f29-f864-40bc-efe6-a97e0f025d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Centroid Cosine Similarity : 0.9912\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fréchet Embedding Distance"
      ],
      "metadata": {
        "id": "m2AID8bSzBuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fed_score = calculate_fed(test_real_embeddings[:2000], test_fake_embeddings[:2000])\n",
        "print(f\"Fréchet Embedding Distance: {fed_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMu91Z4jTBkm",
        "outputId": "d2908028-1163-4e61-c3ca-067e140c8ea6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1256775283.py:9: DeprecationWarning: The `disp` argument is deprecated and will be removed in SciPy 1.18.0.\n",
            "  covmean, _ = sqrtm(cov_1.dot(cov_2) + eps * np.eye(cov_1.shape[0]), disp=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fréchet Embedding Distance: 169.3946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgkTfGSpTm33"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}